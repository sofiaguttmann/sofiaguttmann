---
title: "Practice5S24"
author: "Sofia Guttmann"
date: today
format: pdf
editor: visual
---

```{r}
#| label: load-packages
#| include: false

library(mosaic)
library(tidyverse)
library(mdsr)
library(Lahman)
# add other packages here!


```

# Practice5 - Due Thursday, 3/7 by midnight to Gradescope

Reminder: Practice assignments may be completed working with other individuals.

# Reading

The associated reading for the week is Chapter 19 and Section 12.1.

# Practicing Academic Integrity

If you worked with others or used resources outside of provided course material (anything besides our textbook, course materials in the repo, labs, R help menu) to complete this assignment, please acknowledge them below using a bulleted list.

<!-- ~~~~~~~~~~~~~~~~ YOU MAY BEGIN EDITING BELOW THIS LINE ~~~~~~~~~~~~~~~~ -->

*I acknowledge the following individuals with whom I worked on this assignment:*

Name(s) and corresponding problem(s)

-   

*I used the following sources to help complete this assignment:*

Source(s) and corresponding problem(s)

-   

\newpage

<!-- PROBLEM 1 ---------------------------------------------------------------->

# 1 - MDSR 12.6 (modified)

"Baseball players are voted into the Hall of Fame by the members of the Baseball Writers of America Association. Quantitative criteria are used by the voters, but they are also allowed wide discretion. The following code identifies the position players (not pitchers) who have been elected to the Hall of Fame and tabulates a few basic statistics, include their number of career hits (`tH`), home runs (`tHR`), runs batted in (`tRBI`), and stolen bases (`tSB`)." Only players with more than 1000 total hits are included as a way to obtain the position players only (not pitchers).

```{r}
#| message: false

hof <- Batting %>%
  group_by(playerID) %>%
  inner_join(HallOfFame, by = "playerID") %>%
  filter(inducted == "Y" & votedBy == "BBWAA") %>%
  summarize(tH = sum(H), tHR = sum(HR), tRBI = sum(RBI), tSB = sum(SB)) %>%
  filter(tH > 1000)
```

-   Use the `kmeans()` function to perform a cluster analysis on these players.
-   Explain your choice of $k$, the number of clusters.
-   Describe the properties that seem common to each cluster in your solution.
-   Include at least one visual that helps explore the clusters found.
-   Your solution should include some discussion of whether or not you chose to scale the variables and why. (You should determine whether or not you need to scale before clustering.)
-   Remember that your solution must be reproducible. (Hint: this means you need to do something in your code.)

Solution:

```{r}

library(dplyr)
library(ggplot2)

hof_data <- hof[, c("tH", "tHR", "tRBI", "tSB")]

scaled_data <- scale(hof_data)

wss <- (nrow(scaled_data)-1)*sum(apply(scaled_data,2,var))
for (i in 2:10) wss[i] <- sum(kmeans(scaled_data, centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")

k <- 3
set.seed(123) # For reproducibility
kmeans_result <- kmeans(scaled_data, centers = k)

hof$cluster <- kmeans_result$cluster

cluster_summary <- hof %>%
  group_by(cluster) %>%
  summarize(
    avg_hits = mean(tH),
    avg_home_runs = mean(tHR),
    avg_runs_batted_in = mean(tRBI),
    avg_stolen_bases = mean(tSB),
    players_count = n()
  )

print(cluster_summary)


ggplot(hof, aes(x = tH, y = tHR, color = as.factor(cluster))) +
  geom_point() +
  labs(x = "Total Hits", y = "Total Home Runs", color = "Cluster") +
  theme_minimal()


```

<!-- Have you committed and/or pushed yet? -->

\newpage

<!-- PROBLEM 2 ---------------------------------------------------------------->

# 2 - Trump Tweets

David Robinson, Chief Data Scientist at DataCamp, wrote a blog post ["Text analysis of Trump's tweets confirms he writes only the (angrier) Android half"](http://varianceexplained.org/r/trump-tweets/). He provides a dataset with over 1,500 tweets from the account realDonaldTrump between 12/14/2015 and 8/8/2016. We'll use this dataset to explore the tweeting behavior of @realDonaldTrump during this time period.

First, read in the file. Note that there is a `TwitteR` package which provides an interface to the Twitter web API. We'll use this R dataset David Robinson created using that package so that you don't have to set up Twitter authentication.

```{r}
# the .rda file is also provided if this website ever breaks
load(url("http://varianceexplained.org/files/trump_tweets_df.rda"))
```

> part a - Wrangling! There are a number of variables in the dataset we won't need. First, confirm that all the observations in the dataset are from the screen-name *realDonaldTrump*. Then, create a new dataset called `tweets` that only includes the variables `text`, `created` and `statusSource`.

Solution:

```{r}

load(url("http://varianceexplained.org/files/trump_tweets_df.rda"))

str(trump_tweets_df)

unique(trump_tweets_df$screen_name)  # Check unique screen names

tweets <- trump_tweets_df[, c("text", "created", "statusSource")]

head(tweets)

```

> part b - Using the `statusSource` variable, compute the number of tweets from each source. How many different sources are there? How often are each used?

Hint: You could answer the questions with a nice table printed to the screen.

Solution:

```{r}

tweets_by_source <- table(tweets$statusSource)

num_sources <- length(tweets_by_source)

cat("Number of different sources:", num_sources, "\n")

print(tweets_by_source)


```

> part c - We're going to compare the language used between the Android and iPhone sources, so we only want to keep tweets coming from those sources. Explain what the `extract()` function (from the **tidyverse** package) is doing below. Include in your own words what each argument is doing.

```{r}
#| eval: false # remove eval: false when working on this!

tweets <- tweets %>%
  extract(col = statusSource, into = "source",
          regex = "Twitter for (.*)<",
          remove = FALSE) %>%
  filter(source %in% c("Android", "iPhone"))
```

Solution:

The \`extract()\` function from the \`tidyverse\` package is utilized to isolate a specific pattern from a column within a dataframe and create a new column to store this pattern. Here's a breakdown of each argument:

\`col = statusSource\`: This indicates the column from which the extraction will be carried out. In this context, it refers to the \`statusSource\` column that contains the source information for each tweet.

\`into = "source"\`: This specifies the name of the newly created column where the isolated pattern will be stored. Here, it establishes a new column labeled \`source\`.

\`regex = "Twitter for (.\*)\<"\`: This defines the regular expression pattern used to extract information from the \`statusSource\` column. In this pattern:

\`Twitter for \`: This segment matches the literal phrase "Twitter for ".

\`(.\*)\`: This segment captures any characters (\`.\`) occurring zero or more times (\`\*\`) within parentheses \`()\`. These captured characters constitute the portion of the pattern to be isolated and saved in the new column.

\`\<\`: This matches the literal character "\<" that appears at the end of the source string, signaling the endpoint for the extraction.

\`remove = FALSE\`: This determines whether to retain the original column (\`statusSource\`) following the extraction. By setting it to \`FALSE\`, the original column remains intact within the dataframe.

Subsequent to extracting the source information into the new \`source\` column, the \`filter()\` function is employed to retain only the tweets originating from "Android" or "iPhone" sources. This is accomplished by filtering the dataframe based on whether the \`source\` column contains either "Android" or "iPhone".

> part d - How does the language of the tweets differ by source? Create a word cloud for the top 50 words used in tweets sent from the Android. Create a second word cloud for the top 50 words used in tweets sent from the iPhone. How do these word clouds compare? (Are there some common words frequently used from both sources? Are the most common words different between the sources?)

Note: Don't forget to remove stop words before creating the word cloud. Also remove the terms "https" and "t.co".

Solution:

```{r=FALSE}
library(tm)
library(wordcloud)
library(dplyr)
library(tidytext)


load(url("http://varianceexplained.org/files/trump_tweets_df.rda"))


data(stop_words)


custom_stopwords <- c(stop_words$word, "https", "t.co")


android_tweets <- tweets %>%
  filter(statusSource == "Twitter for Android") %>%
  mutate(text = tolower(text)) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  filter(!word %in% custom_stopwords)

iphone_tweets <- tweets %>%
  filter(statusSource == "Twitter for iPhone") %>%
  mutate(text = tolower(text)) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  filter(!word %in% custom_stopwords)


set.seed(123) # for reproducibility

android_wordcloud_data <- android_tweets %>%
  count(word) %>%
  arrange(desc(n)) %>%
  head(50)

android_wordcloud <- with(android_wordcloud_data, 
                          wordcloud(words = word, freq = n, max.words = 50, colors = brewer.pal(8, "Dark2")))

iphone_wordcloud_data <- iphone_tweets %>%
  count(word) %>%
  arrange(desc(n)) %>%
  head(50)

iphone_wordcloud <- with(iphone_wordcloud_data, 
                         wordcloud(words = word, freq = n, max.words = 50, colors = brewer.pal(8, "Dark2")))


par(mfrow=c(1,2))
android_wordcloud
title(main = "Top 50 Words in Tweets from Android")

iphone_wordcloud
title(main = "Top 50 Words in Tweets from iPhone")



```

```{r}
#| fig-width: 8
#| fig-height: 8


```

> part e - Consider the sentiment. Compute the proportion of words among the tweets within each source classified as "angry" and the proportion of words classified as "joy" based on the NRC lexicon. How does the proportion of "angry" and "joy" words compare between the two sources? What about "positive" and "negative" words?

Solution:

```{r=FALSE}
library(tidyr)


nrc_lexicon <- get_sentiments("nrc")

compute_proportion <- function(tweets_data) {
  tweets_sentiments <- tweets_data %>%
    inner_join(nrc_lexicon, by = "word") %>%
    select(-word) %>%
    group_by(source, sentiment) %>%
    summarize(count = n()) %>%
    pivot_wider(names_from = sentiment, values_from = count, values_fill = 0) %>%
    mutate(total_words = rowSums(across(-source))) %>%
    mutate(across(angry:negative, ~ . / total_words))
  
  return(tweets_sentiments)
}

android_sentiments <- compute_proportion(android_tweets)

iphone_sentiments <- compute_proportion(iphone_tweets)

print("Proportions of words by sentiment in Android tweets:")
print(android_sentiments)

print("Proportions of words by sentiment in iPhone tweets:")
print(iphone_sentiments)

```

> part f - Lastly, based on your responses above, do you think there is evidence to support Robinson's claim that Trump only writes the Android half of the tweets from realDonaldTrump? In 2-4 sentences, please explain.

Solution: There is evidence to suggest that there are differences between tweets posted from Android and iPhone sources. The proportions of certain feelings, such as "angry" and "positive," differ between the two sources, indicating potential variations in the tone and content of tweets. However, further analysis would be necessary to definitively support Robinson's claim that Trump exclusively writes the "angrier" tweets from the Android half.

```{=html}
<!--
Knit, commit, and push, including the final renamed pdf, to your repo. Then, upload the .pdf to Gradescope before the deadline. 
-->
```
